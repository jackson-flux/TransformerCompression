{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43149b44-7089-4bf5-9cd3-4e91b017aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446a1bc-2aee-47a0-8598-edd9c34b85c4",
   "metadata": {},
   "source": [
    "## Set up dimensions\n",
    "All different on purpose, so we can track which one goes where!\n",
    "\n",
    "Here, the `intput_dim` is meant to be the dimension of the input from a model which we want to adapt to use the same embedding space that we've learned in our embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761742cb-6d90-40f8-89e3-476cda735b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "embedding_size = 5\n",
    "vocab_size = 7\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ae647-3c7a-4842-ac82-e2bdc51309a9",
   "metadata": {},
   "source": [
    "## Construct a simple rotation matrix\n",
    "Permutes the first 3 elements. \n",
    "For a column vector $e = (a, b, c, d, e)$, \n",
    "$$Qe = (c, a, b, d, e)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3273170-5d05-44e5-b58e-fe0d8aa597ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.eye(embedding_size)\n",
    "Q[0, 2] = 1\n",
    "Q[1, 1] = 0\n",
    "Q[1, 0] = 1\n",
    "Q[2, 2] = 0\n",
    "Q[2, 1] = 1\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9f6b5-0d65-4f8d-954e-0c0c01c1956b",
   "metadata": {},
   "source": [
    "## Construct an embedding layer and a linear layer which share (part of) an embedding matrix.\n",
    "\n",
    "We construct a torch Embedding layer of with vocab size $v$ and embedding size $e$, whose embedding matrix $E$ will have dimension $(v, e)$.\n",
    "\n",
    "We also take the first `input_dim` elements (rows) of the embedding matrix and use that $(i, e)$ sized sub-matrix to construct a linear layer with input dim $i$ and output dim $e$, the same as the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901275f5-7ba0-4a1b-8542-d0f4dbd48c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lin_from_embed(embedding_layer, input_dim, bias=True):\n",
    "    lin_from_embedding = torch.nn.Linear(input_dim, embedding_layer.weight.shape[-1], bias=bias)\n",
    "    embedding_matrix = embedding_layer.weight.data\n",
    "    # Note: The shape of the weight matrix must be [embedding_size, input_dim], so we must transpose in the next line:\n",
    "    W_ = embedding_matrix.T[:, 0:3]\n",
    "    if bias:\n",
    "        b_ = lin_from_embedding.bias.data\n",
    "        # Transpose to broadcast bias over the columns of W, transpose back to retain the correct shape.\n",
    "        lin_from_embedding.weight.data = (W_.T - b_).T\n",
    "    else:\n",
    "        lin_from_embedding.weight.data = W_\n",
    "    return lin_from_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29af79f5-a4a3-457d-ab99-997a353aaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "lin_from_embedding = get_lin_from_embed(embedding_layer, input_dim, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf697e-b37e-40d9-9193-abab9ec47eed",
   "metadata": {},
   "source": [
    "To check that we have got things right so far, let's inspect the first 2 elements of the embedding matrix, along with the output of the linear layer applied to unit vectors in the x and y directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e64dc1b-e574-46ea-b739-398fc92f0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5562,  0.3084,  0.0092, -1.4830, -0.4561],\n",
       "        [ 1.8453, -0.7494, -0.4230, -1.3185,  0.1440]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_idx = torch.tensor([0, 1])\n",
    "embedding_layer(emb_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e364fec0-8a04-43cc-bc8e-f66108f855af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_input = torch.zeros([batch_size, input_dim])\n",
    "lin_input[0, 0] = 1\n",
    "lin_input[1, 1] = 1\n",
    "lin_input  # Each input is a row vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6962467d-4bb5-4ca4-915e-6f9dcac6cc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5562,  0.3084,  0.0092, -1.4830, -0.4561],\n",
       "        [ 1.8453, -0.7494, -0.4230, -1.3185,  0.1440]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_from_embedding(lin_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01953533-ca95-4424-9571-c49451180815",
   "metadata": {},
   "source": [
    "## Rotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d267111-2f7d-4f4e-8fca-03cf513f206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_embedding(embedding_layer, Q):\n",
    "    # Deepcopy to avoid mutating the original layer.\n",
    "    rotated_embedding_layer = deepcopy(embedding_layer)\n",
    "    W = rotated_embedding_layer.weight.data\n",
    "    rotated_embedding_layer.weight.data = torch.matmul(W, Q)\n",
    "    return rotated_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5b673c0-df30-468f-8504-5ea0717bb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_linear(linear_layer: nn.Linear, Q: torch.Tensor):\n",
    "    rotated_linear_layer = deepcopy(linear_layer)\n",
    "    W_ = rotated_linear_layer.weight.data\n",
    "    rotated_linear_layer.weight.data = torch.matmul(Q.T, W_)\n",
    "    if linear_layer.bias is not None:\n",
    "        b_ = rotated_linear_layer.bias.data\n",
    "        # Either of the next 2 lines work identially:\n",
    "        # rotated_linear_layer.bias.data = torch.matmul(b_, Q)\n",
    "        rotated_linear_layer.bias.data = torch.matmul(Q.T, b_)\n",
    "    return rotated_linear_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5387fbc9-5d48-493f-b210-1850e5bcd906",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_embedding_layer = rotate_embedding(embedding_layer, Q)\n",
    "rotated_lin_from_embedding = rotate_linear(lin_from_embedding, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4508e2-fb51-4032-a03d-b90faf6051ba",
   "metadata": {},
   "source": [
    "Check again that these two outputs match each other, after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f383c60-bddb-4db7-a57f-33445299e943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8646,  0.0092,  0.5562, -1.4830, -0.4561],\n",
       "        [ 1.0960, -0.4230,  1.8453, -1.3185,  0.1440]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_embedding_layer(emb_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b2e061-1375-409c-a734-1d20d16349d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8646,  0.0092,  0.5562, -1.4830, -0.4561],\n",
       "        [ 1.0960, -0.4230,  1.8453, -1.3185,  0.1440]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_lin_from_embedding(lin_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aacfa6-9130-46c5-89fe-477cefb5aa68",
   "metadata": {},
   "source": [
    "# And now, with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89a202e8-2b25-47f5-a4e5-f9aa81a37e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "lin_from_embedding = get_lin_from_embed(embedding_layer, input_dim, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cabbdb-045a-40e3-9fd4-3a64ffc69f16",
   "metadata": {},
   "source": [
    "Double check that the bias broadcasting has worked correctly in the linear layer. The next 2 cells should have identical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c29c9c6c-cef3-4f6c-b3ec-e22c156c1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2411, -0.6772, -0.6461,  0.5904, -1.2008],\n",
       "        [-0.8815,  0.5253,  1.4727, -0.8439,  1.6534]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(emb_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5ced0a9-c3d7-4b70-9698-1dc301978ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2411, -0.6772, -0.6461,  0.5904, -1.2008],\n",
       "        [-0.8815,  0.5253,  1.4727, -0.8439,  1.6534]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_from_embedding(lin_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756b759-ce97-4327-a280-b2c4b1015b9f",
   "metadata": {},
   "source": [
    "But the weight in the linear layer should be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba545778-c1e8-44b0-aa99-b09375db2020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8945, -0.1401, -0.6745,  0.0688, -1.0219],\n",
       "        [-1.2280,  1.0624,  1.4443, -1.3655,  1.8322]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_from_embedding.weight.data.T[0:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a255e-d02c-47ba-8b8e-79c20703e3e3",
   "metadata": {},
   "source": [
    "## Rotating\n",
    "Rotate as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0368d478-6140-4a8e-aa1a-cad42667ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_embedding_layer = rotate_embedding(embedding_layer, Q)\n",
    "rotated_lin_from_embedding = rotate_linear(lin_from_embedding, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958a67b-847d-484f-a543-19e30ea73af8",
   "metadata": {},
   "source": [
    "If the rotation functions above are correct, the next 2 cells should be identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c44bc08-7b5b-4b5b-94e9-ce13f6ff6be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5639, -0.6461,  1.2411,  0.5904, -1.2008],\n",
       "        [-0.3562,  1.4727, -0.8815, -0.8439,  1.6534]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_embedding_layer(emb_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4feb2373-a842-4ea4-b694-405e141e1b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5639, -0.6461,  1.2411,  0.5904, -1.2008],\n",
       "        [-0.3562,  1.4727, -0.8815, -0.8439,  1.6534]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_lin_from_embedding(lin_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246224a-0198-483c-a43f-e9eec191f1c2",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "Let's manually rotate the outputs of the original linear layer, and check them against the outputs of the rotated linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdbeb03f-db1b-4665-a4e7-d30a582bad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.4703484e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [-4.7683716e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 5.9604645e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [-5.9604645e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [-2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([10, input_dim])\n",
    "output = lin_from_embedding(x)\n",
    "manually_rotated_output = torch.matmul(output, Q).detach().numpy()\n",
    "\n",
    "rotated_output = rotated_lin_from_embedding(x).detach().numpy()\n",
    "\n",
    "delta = rotated_output - manually_rotated_output\n",
    "\n",
    "assert np.allclose(manually_rotated_output, rotated_output)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07766a-aa1d-4b9d-a894-dca0e7f9980c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073806f-0405-49ea-91ac-2ae15d191177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmft-env",
   "language": "python",
   "name": "llmft-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
